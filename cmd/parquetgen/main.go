package main

import (
	"flag"
	"log"
	"os"
	"text/template"

	"github.com/parsyl/parquet/internal/parse"
)

var (
	typ = flag.String("type", "", "type name")
	pkg = flag.String("package", "", "package name")
	pth = flag.String("input", "", "path to the go file that defines -type")
)

func main() {
	flag.Parse()

	i := input{
		Package: *pkg,
		Type:    *typ,
	}

	var err error
	var imp string
	i.Fields, imp, err = parse.Fields(*typ, *pth)
	if err != nil {
		log.Fatal(err)
	}

	i.Import = imp

	tmpl, err := template.New("output").Parse(tpl)
	if err != nil {
		log.Fatal(err)
	}

	f, err := os.Create("parquet.go")
	if err != nil {
		log.Fatal(err)
	}

	err = tmpl.Execute(f, i)
	if err != nil {
		log.Fatal(err)
	}

	f.Close()
}

type input struct {
	Package string
	Type    string
	Import  string
	Fields  []string
}

var tpl = `package main

// This code is generated by github.com/parsyl/parquet.

import (
	"bytes"
	"encoding/binary"
	"fmt"
	"io"

	"github.com/golang/snappy"
	"github.com/parsyl/parquet"
	{{.Import}}
)

// ParquetWriter reprents a row group
type ParquetWriter struct {
	fields []Field

	len int

	// child points to the next page
	child *ParquetWriter

	// max is the number of Record items that can get written before
	// a new set of column chunks is written
	max int

	meta *parquet.Metadata
	w    *WriteCounter
}

func Fields() []Field {
	return []Field{ {{range .Fields}}
		{{.}}{{end}}
	}
}

func NewParquetWriter(w io.Writer, opts ...func(*ParquetWriter)) *ParquetWriter {
	p := &ParquetWriter{
		max:    1000,
		w:      &WriteCounter{w: w},
		fields: Fields(),
	}

	for _, opt := range opts {
		opt(p)
	}

	if p.meta == nil {
		ff := Fields()
		schema := make([]parquet.Field, len(ff))
		for i, f := range ff {
			schema[i] = f.Schema()
		}
		p.meta = parquet.New(schema...)
	}

	return p
}

func withMeta(m *parquet.Metadata) func(*ParquetWriter) {
	return func(p *ParquetWriter) {
		p.meta = m
	}
}

// MaxPageSize is the maximum number of rows in each row groups' page.
func MaxPageSize(m int) func(*ParquetWriter) {
	return func(p *ParquetWriter) {
		p.max = m
	}
}

func (p *ParquetWriter) Write() error {
	if _, err := p.w.Write([]byte("PAR1")); err != nil {
		return err
	}

	for i, f := range p.fields {
		pos := p.w.n
		f.Write(p.w, p.meta, pos)

		for child := p.child; child != nil; child = child.child {
			pos := p.w.n
			child.fields[i].Write(p.w, p.meta, pos)
		}
	}

	if err := p.meta.Footer(p.w); err != nil {
		return err
	}

	_, err := p.w.Write([]byte("PAR1"))
	return err
}

func (p *ParquetWriter) Add(rec {{.Type}}) {
	if p.len == p.max {
		if p.child == nil {
			p.child = NewParquetWriter(p.w, MaxPageSize(p.max), withMeta(p.meta))
		}

		p.child.Add(rec)
		return
	}

	for _, f := range p.fields {
		f.Add(rec)
	}

	p.len++
}

type Field interface {
	Add(r {{.Type}})
	Write(w io.Writer, meta *parquet.Metadata, pos int64) error
	Schema() parquet.Field
	Scan(r *{{.Type}})
	Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error
	Name() string
}

type RequiredField struct {
	col string
}

func (f *RequiredField) doWrite(w io.Writer, meta *parquet.Metadata, pos int64, vals []byte, count int) error {
	compressed := snappy.Encode(nil, vals)
	if err := meta.WritePageHeader(w, f.col, pos, len(vals), len(compressed), count); err != nil {
		return err
	}

	_, err := w.Write(compressed)
	return err
}

func (f *RequiredField) doRead(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) (io.Reader, error) {
	var nRead int
	var out []byte

	for nRead < pos.N {
		ph, err := meta.PageHeader(r)
		if err != nil {
			return nil, err
		}

		compressed := make([]byte, ph.CompressedPageSize)
		if _, err := r.Read(compressed); err != nil {
			return nil, err
		}

		data, err := snappy.Decode(nil, compressed)
		out = append(out, data...)
		nRead += int(ph.DataPageHeader.NumValues)
	}

	return bytes.NewBuffer(out), nil
}

func (f *RequiredField) Name() string {
	return f.col
}

type OptionalField struct {
	defs []int64
	col  string
}

func (f *OptionalField) nVals() int {
	var out int
	for _, d := range f.defs {
		if d == 1 {
			out++
		}
	}
	return out
}

func (f *OptionalField) doWrite(w io.Writer, meta *parquet.Metadata, pos int64, vals []byte, count int) error {
	buf := bytes.Buffer{}
	wc := &WriteCounter{w: &buf}

	err := parquet.WriteLevels(wc, f.defs)
	if err != nil {
		return err
	}

	if _, err := wc.Write(vals); err != nil {
		return err
	}

	compressed := snappy.Encode(nil, buf.Bytes())
	if err := meta.WritePageHeader(w, f.col, pos, int(wc.n), len(compressed), len(f.defs)); err != nil {
		return err
	}

	_, err = w.Write(compressed)
	return err
}

func (f *OptionalField) doRead(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) (io.Reader, error) {
	var nRead int
	var out []byte

	for nRead < pos.N {
		ph, err := meta.PageHeader(r)
		if err != nil {
			return nil, err
		}

		compressed := make([]byte, ph.CompressedPageSize)
		if _, err := r.Read(compressed); err != nil {
			return nil, err
		}

		uncompressed, err := snappy.Decode(nil, compressed)
		if err != nil {
			return nil, err
		}

		defs, l, err := parquet.ReadLevels(bytes.NewBuffer(uncompressed))
		if err != nil {
			return nil, err
		}
		f.defs = append(f.defs, defs...)
		out = append(out, uncompressed[l:]...)
		nRead += int(ph.DataPageHeader.NumValues)
	}

	return bytes.NewBuffer(out), nil
}

func (f *OptionalField) Name() string {
	return f.col
}

type Uint32Field struct {
	vals []uint32
	RequiredField
	val  func(r {{.Type}}) uint32
	read func(r *{{.Type}}, v uint32)
}

func NewUint32Field(val func(r {{.Type}}) uint32, read func(r *{{.Type}}, v uint32), col string) *Uint32Field {
	return &Uint32Field{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *Uint32Field) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Uint32Type, RepetitionType: parquet.RepetitionRequired}
}

func (f *Uint32Field) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *Uint32Field) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Uint32Field) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]uint32, int(pos.N))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Uint32Field) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

type Uint32OptionalField struct {
	OptionalField
	vals []uint32
	read func(r *{{.Type}}, v *uint32)
	val  func(r {{.Type}}) *uint32
}

func NewUint32OptionalField(val func(r {{.Type}}) *uint32, read func(r *{{.Type}}, v *uint32), col string) *Uint32OptionalField {
	return &Uint32OptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *Uint32OptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Uint32Type, RepetitionType: parquet.RepetitionOptional}
}

func (f *Uint32OptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Uint32OptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]uint32, f.nVals())
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Uint32OptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

func (f *Uint32OptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *uint32
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

type Int32Field struct {
	vals []int32
	RequiredField
	val  func(r {{.Type}}) int32
	read func(r *{{.Type}}, v int32)
}

func NewInt32Field(val func(r {{.Type}}) int32, read func(r *{{.Type}}, v int32), col string) *Int32Field {
	return &Int32Field{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *Int32Field) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Int32Type, RepetitionType: parquet.RepetitionRequired}
}

func (f *Int32Field) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	if err := binary.Write(&buf, binary.LittleEndian, f.vals); err != nil {
		return err
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Int32Field) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]int32, int(pos.N))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Int32Field) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

func (f *Int32Field) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

type Int32OptionalField struct {
	vals []int32
	OptionalField
	val  func(r {{.Type}}) *int32
	read func(r *{{.Type}}, v *int32)
}

func NewInt32OptionalField(val func(r {{.Type}}) *int32, read func(r *{{.Type}}, v *int32), col string) *Int32OptionalField {
	return &Int32OptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *Int32OptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Int32Type, RepetitionType: parquet.RepetitionOptional}
}

func (f *Int32OptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *int32
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *Int32OptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Int32OptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]int32, f.nVals())
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Int32OptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

type Int64Field struct {
	vals []int64
	RequiredField
	val  func(r {{.Type}}) int64
	read func(r *{{.Type}}, v int64)
}

func NewInt64Field(val func(r {{.Type}}) int64, read func(r *{{.Type}}, v int64), col string) *Int64Field {
	return &Int64Field{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *Int64Field) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Int64Type, RepetitionType: parquet.RepetitionRequired}
}

func (f *Int64Field) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *Int64Field) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Int64Field) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]int64, int(pos.N))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Int64Field) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

type Int64OptionalField struct {
	vals []int64
	OptionalField
	val  func(r {{.Type}}) *int64
	read func(r *{{.Type}}, v *int64)
}

func NewInt64OptionalField(val func(r {{.Type}}) *int64, read func(r *{{.Type}}, v *int64), col string) *Int64OptionalField {
	return &Int64OptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *Int64OptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Int64Type, RepetitionType: parquet.RepetitionOptional}
}

func (f *Int64OptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Int64OptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]int64, int(f.nVals()))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Int64OptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *int64
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *Int64OptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

type Uint64Field struct {
	vals []uint64
	RequiredField
	val  func(r {{.Type}}) uint64
	read func(r *{{.Type}}, v uint64)
}

func NewUint64Field(val func(r {{.Type}}) uint64, read func(r *{{.Type}}, v uint64), col string) *Uint64Field {
	return &Uint64Field{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *Uint64Field) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Uint64Type, RepetitionType: parquet.RepetitionRequired}
}

func (f *Uint64Field) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Uint64Field) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]uint64, int(pos.N))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Uint64Field) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *Uint64Field) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

type Uint64OptionalField struct {
	vals []uint64
	OptionalField
	val  func(r {{.Type}}) *uint64
	read func(r *{{.Type}}, v *uint64)
}

func NewUint64OptionalField(val func(r {{.Type}}) *uint64, read func(r *{{.Type}}, v *uint64), col string) *Uint64OptionalField {
	return &Uint64OptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *Uint64OptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Uint64Type, RepetitionType: parquet.RepetitionOptional}
}

func (f *Uint64OptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Uint64OptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]uint64, int(f.nVals()))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Uint64OptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *uint64
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *Uint64OptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

type Float32Field struct {
	vals []float32
	RequiredField
	val  func(r {{.Type}}) float32
	read func(r *{{.Type}}, v float32)
}

func NewFloat32Field(val func(r {{.Type}}) float32, read func(r *{{.Type}}, v float32), col string) *Float32Field {
	return &Float32Field{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *Float32Field) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Float32Type, RepetitionType: parquet.RepetitionRequired}
}

func (f *Float32Field) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Float32Field) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]float32, int(pos.N))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Float32Field) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *Float32Field) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

type Float32OptionalField struct {
	vals []float32
	OptionalField
	val  func(r {{.Type}}) *float32
	read func(r *{{.Type}}, v *float32)
}

func NewFloat32OptionalField(val func(r {{.Type}}) *float32, read func(r *{{.Type}}, v *float32), col string) *Float32OptionalField {
	return &Float32OptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *Float32OptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.Float32Type, RepetitionType: parquet.RepetitionOptional}
}

func (f *Float32OptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	var buf bytes.Buffer
	for _, v := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, v); err != nil {
			return err
		}
	}
	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *Float32OptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	v := make([]float32, int(f.nVals()))
	err = binary.Read(rr, binary.LittleEndian, &v)
	f.vals = v
	return err
}

func (f *Float32OptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *float32
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *Float32OptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)

		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

type BoolField struct {
	RequiredField
	vals []bool
	val  func(r {{.Type}}) bool
	read func(r *{{.Type}}, v bool)
}

func NewBoolField(val func(r {{.Type}}) bool, read func(r *{{.Type}}, v bool), col string) *BoolField {
	return &BoolField{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *BoolField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.BoolType, RepetitionType: parquet.RepetitionRequired}
}

func (f *BoolField) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *BoolField) Name() string {
	return f.col
}

func (f *BoolField) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

func (f *BoolField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	ln := len(f.vals)
	byteNum := (ln + 7) / 8
	rawBuf := make([]byte, byteNum)

	for i := 0; i < ln; i++ {
		if f.vals[i] {
			rawBuf[i/8] = rawBuf[i/8] | (1 << uint32(i%8))
		}
	}

	return f.doWrite(w, meta, pos, rawBuf, len(f.vals))
}

func (f *BoolField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	f.vals, err = parquet.GetBools(rr, int(pos.N))
	return err
}

type BoolOptionalField struct {
	OptionalField
	vals []bool
	val  func(r {{.Type}}) *bool
	read func(r *{{.Type}}, v *bool)
}

func NewBoolOptionalField(val func(r {{.Type}}) *bool, read func(r *{{.Type}}, v *bool), col string) *BoolOptionalField {
	return &BoolOptionalField{
		val:           val,
		read:          read,
		OptionalField: OptionalField{col: col},
	}
}

func (f *BoolOptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.BoolType, RepetitionType: parquet.RepetitionOptional}
}

func (f *BoolOptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	f.vals, err = parquet.GetBools(rr, f.nVals())
	return err
}

func (f *BoolOptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *bool
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *BoolOptionalField) Name() string {
	return f.col
}

func (f *BoolOptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

func (f *BoolOptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	ln := len(f.vals)
	byteNum := (ln + 7) / 8
	rawBuf := make([]byte, byteNum)

	for i := 0; i < ln; i++ {
		if f.vals[i] {
			rawBuf[i/8] = rawBuf[i/8] | (1 << uint32(i%8))
		}
	}

	return f.doWrite(w, meta, pos, rawBuf, len(f.vals))
}

type StringField struct {
	RequiredField
	vals []string
	val  func(r {{.Type}}) string
	read func(r *{{.Type}}, v string)
}

func NewStringField(val func(r {{.Type}}) string, read func(r *{{.Type}}, v string), col string) *StringField {
	return &StringField{
		val:           val,
		read:          read,
		RequiredField: RequiredField{col: col},
	}
}

func (f *StringField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.StringType, RepetitionType: parquet.RepetitionRequired}
}

func (f *StringField) Scan(r *{{.Type}}) {
	v := f.vals[0]
	f.vals = f.vals[1:]
	f.read(r, v)
}

func (f *StringField) Name() string {
	return f.col
}

func (f *StringField) Add(r {{.Type}}) {
	f.vals = append(f.vals, f.val(r))
}

func (f *StringField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	buf := bytes.Buffer{}

	for _, s := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, int32(len(s))); err != nil {
			return err
		}
		buf.Write([]byte(s))
	}

	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *StringField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	for j := 0; j < pos.N; j++ {
		var x int32
		if err := binary.Read(rr, binary.LittleEndian, &x); err != nil {
			return err
		}
		s := make([]byte, x)
		if _, err := rr.Read(s); err != nil {
			return err
		}

		f.vals = append(f.vals, string(s))
	}
	return nil
}

type StringOptionalField struct {
	OptionalField
	vals []string
	val  func(r {{.Type}}) *string
	read func(r *{{.Type}}, v *string)
}

func NewStringOptionalField(val func(r {{.Type}}) *string, read func(r *{{.Type}}, v *string), col string) *StringOptionalField {
	return &StringOptionalField{
		val:  val,
		read: read,
		OptionalField: OptionalField{
			col: col,
		},
	}
}

func (f *StringOptionalField) Schema() parquet.Field {
	return parquet.Field{Name: f.col, Type: parquet.StringType, RepetitionType: parquet.RepetitionOptional}
}

func (f *StringOptionalField) Scan(r *{{.Type}}) {
	if len(f.defs) == 0 {
		return
	}

	var val *string
	if f.defs[0] == 1 {
		v := f.vals[0]
		f.vals = f.vals[1:]
		val = &v
	}
	f.defs = f.defs[1:]
	f.read(r, val)
}

func (f *StringOptionalField) Name() string {
	return f.col
}

func (f *StringOptionalField) Add(r {{.Type}}) {
	v := f.val(r)
	if v != nil {
		f.vals = append(f.vals, *v)
		f.defs = append(f.defs, 1)
	} else {
		f.defs = append(f.defs, 0)
	}
}

func (f *StringOptionalField) Write(w io.Writer, meta *parquet.Metadata, pos int64) error {
	buf := bytes.Buffer{}

	for _, s := range f.vals {
		if err := binary.Write(&buf, binary.LittleEndian, int32(len(s))); err != nil {
			return err
		}
		buf.Write([]byte(s))
	}

	return f.doWrite(w, meta, pos, buf.Bytes(), len(f.vals))
}

func (f *StringOptionalField) Read(r io.ReadSeeker, meta *parquet.Metadata, pos parquet.Position) error {
	rr, err := f.doRead(r, meta, pos)
	if err != nil {
		return err
	}

	for j := 0; j < pos.N; j++ {
		if f.defs[0] == 0 {
			continue
		}

		var x int32
		if err := binary.Read(rr, binary.LittleEndian, &x); err != nil {
			return err
		}
		s := make([]byte, x)
		if _, err := r.Read(s); err != nil {
			return err
		}

		f.vals = append(f.vals, string(s))
	}
	return nil
}

type WriteCounter struct {
	n int64
	w io.Writer
}

func (w *WriteCounter) Write(p []byte) (int, error) {
	n, err := w.w.Write(p)
	w.n += int64(n)
	return n, err
}

type ReadCounter struct {
	n int64
	r io.ReadSeeker
}

func (r *ReadCounter) Seek(o int64, w int) (int64, error) {
	return r.r.Seek(o, w)
}

func (r *ReadCounter) Read(p []byte) (int, error) {
	n, err := r.r.Read(p)
	r.n += int64(n)
	return n, err
}

func NewParquetReader(r io.ReadSeeker, opts ...func(*ParquetReader)) (*ParquetReader, error) {
	ff := Fields()
	pr := &ParquetReader{
		fields: ff,
		r:      r,
	}

	for _, opt := range opts {
		opt(pr)
	}

	if pr.meta == nil {

		schema := make([]parquet.Field, len(ff))
		for i, f := range ff {
			schema[i] = f.Schema()
		}

		pr.meta = parquet.New(schema...)

		if err := pr.meta.ReadFooter(r); err != nil {
			return nil, err
		}
		pr.rows = pr.meta.Rows()
		pr.offsets = pr.meta.Offsets()
	}

	_, err := r.Seek(4, io.SeekStart)
	if err != nil {
		return nil, err
	}

	for _, f := range pr.fields {
		offsets := pr.offsets[f.Name()]
		if len(offsets) <= pr.index {
			break
		}

		o := offsets[pr.index] //TODO: increment index to get more row groups
		if err := f.Read(r, pr.meta, o); err != nil {
			return nil, fmt.Errorf("unable to read field %s, err: %s", f.Name(), err)
		}
	}

	return pr, nil

}

func readerIndex(i int) func(*ParquetReader) {
	return func(p *ParquetReader) {
		p.index = i
	}
}

func readerMeta(m *parquet.Metadata) func(*ParquetReader) {
	return func(p *ParquetReader) {
		p.meta = m
	}
}

// ParquetReader reads one page from a row group.
type ParquetReader struct {
	fields []Field

	index   int
	cur     int64
	rows    int64
	offsets map[string][]parquet.Position

	r    io.ReadSeeker
	meta *parquet.Metadata
}

func (p *ParquetReader) Next() bool {
	if p.cur >= p.rows {
		return false
	}
	p.cur++
	return true
}

func (p *ParquetReader) Scan(x *{{.Type}}) {
	for _, f := range p.fields {
		f.Scan(x)
	}
}`
